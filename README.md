# ğŸ” SPW: Search-based Preference Weighting

This repository implements **SPW (Search-based Preference Weighting)** â€” a unified single-stage framework for learning reward functions from both demonstrations and preference feedback.

```plaintext
ğŸ“¦ SPW
â”œâ”€â”€ algorithms/                  # RL algorithms
â”‚   â”œâ”€â”€ BC.py                    # Behavior Cloning (BC)

â”‚   â”œâ”€â”€ iql.py                   # IQL policy learning

â”‚   â””â”€â”€ utils_env.py             # Environment & dataset utilities

â”œâ”€â”€ Reward_learning/             # Reward model components
â”‚   â”œâ”€â”€ learn_reward.py          # Train reward model
â”‚   â”œâ”€â”€ reward_model.py          # Reward model architecture
â”‚   â””â”€â”€ reward_utils.py          # Reward learning utilities
â”‚
â”œâ”€â”€ configs/                     # YAML config files
â”‚   â”œâ”€â”€ bc.yaml

â”‚   â”œâ”€â”€ iql.yaml

â”‚   â””â”€â”€ reward.yaml
â”‚
â”œâ”€â”€ dataset/                     # MetaWorld & DMC datasets
â”‚
â”œâ”€â”€ preference_datasets/         # Optional human preference data
â”‚
â”œâ”€â”€ scripts/                     # Example scripts (e.g., run experiments)
â”‚
â”œâ”€â”€ SPW.yml                      # Conda environment file
â”‚
â””â”€â”€ README.md


## âš™ï¸ Installation

Create a conda environment and install dependencies:

```bash
conda env create -f SPW.yml
pip install git+https://github.com/Farama-Foundation/Metaworld.git@master#egg=metaworld
pip install git+https://github.com/denisyarats/dmc2gym.git
```

ğŸš€ Quick Start

1ï¸âƒ£ Train the Reward Model (SPW mode)

```bash
python Reward_learning/learn_reward.py \
  --config=configs/reward.yaml \
  --env=metaworld_box-close-v2 \
  --mode=SPW \
  --spw_tau=0.7
```

2ï¸âƒ£ Run IQL with SPW Reward

```bash
python algorithms/iql.py \
  --config=configs/iql.yaml \
  --use_reward_model=True \
  --env=metaworld_box-close-v2 \
  --mode=SPW \
  --spw_tau=0.7
```

Or run the full pipeline with:

```bash
bash scripts/example.sh
```

# ğŸ“Œ Algorithms

In this repo, we can run the following reward-learning methods:

- **MR** â€“ MLP Reward Model  
- **BC-P** â€“ Behavior Cloning Pretraining  
- **R-P** â€“ Reward Pretraining  
- **RD** â€“ Reward Distribution  
- **D-REX** â€“ Disturbance-based Reward Extrapolation  
- **SPW** â€“ Search-based Preference Weighting  

For other baselines, we experimented with the following repositories:

| Algorithm | URL |
|----------|-----|
| **OPRL** (Offline Preference-based Reward Learning) | https://github.com/danielshin1/oprl |
| **PT** (Preference Transformer) | https://github.com/csmile-1006/PreferenceTransformer |
| **IPL** (Inverse Preference Learning) | https://github.com/jhejna/inverse-preference-learning |
| **LiRE** (Listwise Reward Estimation) | https://github.com/chwoong/LiRE |
