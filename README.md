# ğŸ” SPW: Search-based Preference Weighting

This repository implements **SPW (Search-based Preference Weighting)**, a reward learning method for enhancing reinforcement learning in **MetaWorld** and **DMC** environments.

---

ğŸ“‚ Project Structure

â”œâ”€â”€ algorithms/                  # RL algorithms

â”‚   â”œâ”€â”€ BC.py                    # Behavior Cloning (BC)

â”‚   â”œâ”€â”€ iql.py                   # IQL policy learning

â”‚   â””â”€â”€ utils_env.py             # Environment & dataset utilities

â”‚
â”œâ”€â”€ Reward_learning/             # Reward model components

â”‚   â”œâ”€â”€ learn_reward.py          # Entry point for training reward model

â”‚   â”œâ”€â”€ reward_model.py          # Reward model architecture

â”‚   â””â”€â”€ reward_utils.py          # Helper functions for reward learning

â”‚

â”œâ”€â”€ configs/                     # YAML configuration files

â”‚   â”œâ”€â”€ bc.yaml

â”‚   â”œâ”€â”€ iql.yaml

â”‚   â””â”€â”€ reward.yaml

â”‚

â”œâ”€â”€ dataset/                     # MetaWorld & DMC datasets

â”‚

â”œâ”€â”€ preference_datasets/         # Optional human preference data

â”‚

â”œâ”€â”€ scripts/                     # Example scripts (e.g., example.sh)

â”‚

â”œâ”€â”€ spw.yml                      # Conda environment file

â”‚

â””â”€â”€ README.md

## âš™ï¸ Installation

Create a conda environment and install dependencies:

```bash
conda env create -f SPW.yml
pip install git+https://github.com/Farama-Foundation/Metaworld.git@master#egg=metaworld
pip install git+https://github.com/denisyarats/dmc2gym.git
```

ğŸš€ Quick Start

1ï¸âƒ£ Train the Reward Model (SPW mode)

```bash
python Reward_learning/learn_reward.py \
  --config=configs/reward.yaml \
  --env=metaworld_box-close-v2 \
  --mode=SPW \
  --spw_tau=0.7
```

2ï¸âƒ£ Run IQL with SPW Reward

```bash
python algorithms/iql.py \
  --config=configs/iql.yaml \
  --use_reward_model=True \
  --env=metaworld_box-close-v2 \
  --mode=SPW \
  --spw_tau=0.7
```

Or run the full pipeline with:

```bash
bash scripts/example.sh
```

ğŸ“Œ Notes
Supported modes:
MR â€“ MLP Reward Model
BC-P â€“ Behavior Cloning Pretraining
R-P â€“ Reward Pretraining
RD â€“ Reward Distribution
D-REX â€“ Disturbance-based Reward Extrapolation
SPW â€“ Search-based Preference Weighting

Modify hyperparameters via the YAML config files in configs/.
